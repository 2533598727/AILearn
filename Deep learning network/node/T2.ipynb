{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  手写一个浅层神经网络\n",
    "1. 定义网络结构\n",
    "2. 初始化参数\n",
    "3. 循环\n",
    "    1. 前向传播\n",
    "    2. 计算损失\n",
    "    3. 反向传播\n",
    "    4. 更新参数\n",
    "4. 结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的样本数 209\n",
      "测试集的样本数 50\n",
      "train_set_y的维度 (1, 209)\n",
      "test_set_y的维度 (1, 50)\n",
      "train_set_x_orig的维度 (209, 64, 64, 3)\n",
      "test_set_x_orig的维度 (50, 64, 64, 3)\n",
      "(12288, 209) (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import data\n",
    "load_dataset = data.load_dataset\n",
    "\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "print(\"训练集的样本数\", train_set_x_orig.shape[0])\n",
    "print(\"测试集的样本数\", test_set_x_orig.shape[0])\n",
    "print(\"train_set_y的维度\", train_set_y.shape)\n",
    "print(\"test_set_y的维度\", test_set_y.shape)\n",
    "print(\"train_set_x_orig的维度\", train_set_x_orig.shape)\n",
    "print(\"test_set_x_orig的维度\", test_set_x_orig.shape)\n",
    "#转换成一维向量\n",
    "train_x = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_x = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "print(train_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#sigmoid function \n",
    "def sigmoid(x):\n",
    "    # 对大的正值和负值进行特殊处理，避免溢出\n",
    "     # 限制输入范围，防止exp计算溢出\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#sigmoid derivative\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))  # Derivative of sigmoid function: f'(x) = f(x) * (1 - f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X,Y):\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4  # 假设隐藏层有4个神经元\n",
    "    n_y = Y.shape[0]\n",
    "    return (n_x,n_h,n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_param(n_x,n_h,n_y):\n",
    "    np.random.seed(2)\n",
    "    #创建 隐藏层参数\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    #创建 输出层参数\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "\n",
    "    #检查\n",
    "    assert(W1.shape == (n_h,n_x))\n",
    "    assert(b1.shape == (n_h,1))\n",
    "    assert(W2.shape == (n_y,n_h))\n",
    "    assert(b2.shape == (n_y,1))\n",
    "\n",
    "    parameters = {\"W1\":W1,\n",
    "    \"b1\":b1,\n",
    "    \"W2\":W2,\n",
    "    \"b2\":b2}\n",
    "\n",
    "    print(W1.shape,b1.shape,W2.shape,b2.shape)\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "            \"A1\": A1,\n",
    "            \"Z2\": Z2,\n",
    "            \"A2\": A2}\n",
    "\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算损失\n",
    "多样本计算交叉熵损失 求和求平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loss(A2,Y):\n",
    "    m = Y.shape[1]\n",
    "    J = -1/m*np.sum(np.multiply(np.log(A2),Y) + np.multiply((1 - Y),np.log(1 - A2))) #最大化对数似然 -> 最小化损失函数\n",
    "    cost = np.squeeze(J) \n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache,parameters):\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    dZ2 = A2 - Y #公式推到得到\n",
    "    dW2 = 1/m * np.dot(dZ2,A1.T) \n",
    "    db2 = 1/m * np.sum(dZ2,axis = 1,keepdims = True)\n",
    "    dZ1 = W2.T * dZ2 * (1- np.power(A1,2))\n",
    "    dW1 = 1/m * np.dot(dZ1,X.T)\n",
    "    db1 = 1/m * np.sum(dZ1,axis = 1,keepdims = True)\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立网络模型训练逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,Y,num_iterations = 10000,learning_rate=0.05,print_cost = False):\n",
    "    n_x,n_h,n_y = layer_sizes(X,Y) #定义网格结构\n",
    "    parameters = init_param(n_x,n_h,n_y) #初始化参数\n",
    "    for i in range(0,num_iterations): #迭代训练\n",
    "        A2,cache = propagate(X,parameters) #前向传播\n",
    "        cost = count_loss(A2,Y) #计算损失\n",
    "        grads = backward_propagation(X,Y,cache,parameters) #反向传播\n",
    "        parameters  = update_parameters(parameters,grads,learning_rate) #更新参数\n",
    "        if i % 100 == 0 and print_cost: #每1000次打印损失\n",
    "            print(\"第%i代: %f\" %(i, cost))\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters,X):\n",
    "    A2,cache = propogate(parameters,X) #前向传播预测概率\n",
    "    predictions = np.array(\n",
    "        [1 if A2[0,i] > 0.5 else 0 for i in range(A2.shape[1])]\n",
    "    ).reshape(A2.shape) #概率大于0.5为1，小于0.5为0\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 12288) (4, 1) (1, 4) (1, 1)\n",
      "第0代: 0.696842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第100代: 0.662961\n",
      "第200代: 0.649107\n",
      "第300代: 0.644402\n",
      "第400代: 0.642669\n",
      "第500代: 0.641921\n",
      "第600代: 0.641510\n",
      "第700代: 0.641218\n",
      "第800代: 0.640972\n",
      "第900代: 0.640746\n",
      "准确率: 34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mi\\AppData\\Local\\Temp\\ipykernel_18976\\1310979653.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('准确率: %d' % float((np.dot(test_set_y,predictions.T) + np.dot(1-test_set_y,1-predictions.T))/float(test_set_y.size)*100) + '%')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_iterations = 1000\n",
    "learning_rate = 0.005\n",
    "parameters  = model(train_x, train_set_y, num_iterations, learning_rate,True)\n",
    "predictions = predict(test_x,parameters)\n",
    "print('准确率: %d' % float((np.dot(test_set_y,predictions.T) + np.dot(1-test_set_y,1-predictions.T))/float(test_set_y.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
